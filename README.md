# Preference-Based Fine-Tuning of Genomic Sequence Models for Personal Expression Prediction with Data Augmentation

This repository contains the official implementation for the study:

> **Preference-Based Fine-Tuning of Genomic Sequence Models for Personal Expression Prediction with Data Augmentation**

It provides preprocessing pipelines, training scripts, and evaluation tools for fine-tuning Enformer-based architectures on both real and simulated genomic data.

---

## üìò Overview

This project introduces a **hybrid fine-tuning framework** that integrates real RNA-seq‚Äìbased regression with simulated genome‚Äìbased preference learning.  
The method leverages **LoRA-based adapters** for parameter-efficient fine-tuning of Enformer models and uses population-level genetic diversity simulated via `sim1000G` to enhance cross-individual generalization.

---

## üß¨ Data Preparation

All datasets used in this study can be reproduced by following the scripts in the [`data_preprocessing`](./data_preprocessing) directory.

### 1. Prepare Real Sequences
Scripts:
- `/data_preprocessing/1.preparing_real_sequences/1.vcf_to_fasta.py`
- `/data_preprocessing/1.preparing_real_sequences/3.fasta_to_vector.py`

### 2. Prepare Virtual (Simulated) Sequences
Scripts:
- `/data_preprocessing/2.preparing_virtual_sequences/7.vcf_to_fasta.py`
- `/data_preprocessing/2.preparing_virtual_sequences/9.fasta_to_vector.py`

Simulated genomes (1,000 individuals / 2,000 haplotypes) were generated using [`sim1000G`](https://adimitromanolakis.github.io/sim1000G/inst/doc/SimulatingFamilyData.html).

You can download **FASTA sequence files generated by `1.vcf_to_fasta.py` and `7.vcf_to_fasta.py`**  
(for both real and virtual individuals) from Zenodo:

üëâ [Zenodo dataset link](https://zenodo.org/records/17510022)

(Please make sure to use the v2 release of the Zenodo dataset)

These files contain **reference-based (real) and simulated (virtual) personal genomes in FASTA format**.  
After downloading, you must **convert these FASTA files into vectorized tensor representations** using the following scripts:
- `/data_preprocessing/1.preparing_real_sequences/3.fasta_to_vector.py`
- `/data_preprocessing/2.preparing_virtual_sequences/9.fasta_to_vector.py`
---

## ‚öôÔ∏è Environment Setup
To run the codes in finetuning_enformer, refer to the [`environment_setup`](./environment_setup) directory for installation and environment configuration instructions.
  
## üöÄ Running Experiments
First, download
https://github.com/lucidrains/enformer-pytorch/blob/main/enformer_pytorch/precomputed/tf_gammas.pt
and place it in:
```bash
finetuning_enformer/enformer_pytorch_for_lora/precomputed/tf_gammas.pt
```
The main fine-tuning experiments are located in the finetuning_enformer directory.
Before launching experiments, please update all dataset and checkpoint paths in:
```bash
train_enformer/config.py
```
This file contains clearly organized fields for real-data paths, synthetic-data paths,
and output directory locations. You must replace these with paths valid for your system.
All required input files are located in the data/ directory, making it easy to match
each field in config.py with the corresponding dataset.
In particular, the fine-tuning pipeline requires the outputs of:
```bash
data_preprocessing/1.preparing_real_sequences/3.fasta_to_vector.py
data_preprocessing/2.preparing_virtual_sequences/9.fasta_to_vector.py
```
These scripts convert real and virtual personal-genome FASTA files into vectorized
tensor representations used for training.
To launch all experiments in the suite simultaneously (as run in our study on 4√óH100 GPUs):
```bash
deepspeed --num_nodes 1 --num_gpus 4 \
    --master_addr ${MASTER_ADDR} \
    --master_port ${MASTER_PORT} \
    --module train_enformer.main -- --suite
```
Environment variables MASTER_ADDR and MASTER_PORT can be set manually if needed
(e.g., when multiple distributed jobs are running on the same machine).

This command automatically executes all experiment phases, including:

‚Ä¢ Real-only regression

‚Ä¢ Real‚Äìsynthetic alternating fine-tuning

‚Ä¢ Real-only fine-tuning combining regression and Bradley‚ÄìTerry preference objectives


## üìä Evaluation
After completing a training experiment, you can evaluate the model on the test set by running:
```bash
python evaluation_on_testset.py
```
This script loads the best-performing checkpoint (selected based on validation Pearson correlation), computes per-gene Pearson and Spearman correlations between predicted and observed expression values across individuals, and outputs both gene-level and summary metrics.


## üìÑ Citation & Licenses

This repository integrates components from several open-source projects under the MIT License:
```bash
Copyright (c) 2018 Kipoi team
Licensed under the MIT License.
Source: https://github.com/kipoi/kipoiseq

Copyright (c) 2021 Phil Wang
Licensed under the MIT License.
Source: https://github.com/lucidrains/enformer-pytorch

Copyright (c) 2023 ni-lab
Licensed under the MIT License.
Source: https://github.com/ni-lab/personalized-expression-benchmark
```

All sequences used in this study were derived from [E-GEUV-1](https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1). The virtual genomes generated using sim1000G were also simulated based on this dataset as input.
