# Preference-Based Fine-Tuning of Genomic Sequence Models for Personal Expression Prediction with Data Augmentation

This repository contains the official implementation for the study:

> **Preference-Based Fine-Tuning of Genomic Sequence Models for Personal Expression Prediction with Data Augmentation**

It provides preprocessing pipelines, training scripts, and evaluation tools for fine-tuning Enformer-based architectures on both real and simulated genomic data.

---

## üìò Overview

This project introduces a **hybrid fine-tuning framework** that integrates real RNA-seq‚Äìbased regression with simulated genome‚Äìbased preference learning.  
The method leverages **LoRA-based adapters** for parameter-efficient fine-tuning of Enformer models and uses population-level genetic diversity simulated via `sim1000G` to enhance cross-individual generalization.

---
## ‚öôÔ∏è Environment Setup
To run the codes in finetuning_enformer, refer to the [`environment_setup`](./environment_setup) directory for installation and environment configuration instructions.

---
## üß¨ Data Preparation

All datasets used in this study can be reproduced by following the scripts in the [`data_preprocessing`](./data_preprocessing) directory.

You can download **FASTA sequence files generated by `1.vcf_to_fasta.py` and `7.vcf_to_fasta.py`**  
(for both real and synthetic individuals) from Zenodo:

üëâ [Zenodo dataset link](https://zenodo.org/records/17510022)

(Please make sure to use the v2 release of the Zenodo dataset)

These files contain **reference-based (real) and simulated (synthetic) personal genomes in FASTA format**.  
After downloading, you must **convert these FASTA files into vectorized tensor representations** using the following scripts:
```bash
python /data_preprocessing/1.preparing_real_sequences/3.fasta_to_vector.py
python /data_preprocessing/2.preparing_synthetic_sequences/9.fasta_to_vector.py
```
Once you have run the scripts above, all other code under `/data_preprocessing` can be ignored‚Äîthose scripts are only used to generate the files already provided in the `data/` directory.
You can simply use the existing files in the `data/` directory by specifying their paths in `train_enformer/config.py`.

---
  
## üöÄ Running Experiments
First, download
https://github.com/lucidrains/enformer-pytorch/blob/main/enformer_pytorch/precomputed/tf_gammas.pt
and place it in:
```bash
finetuning_enformer/enformer_pytorch_for_lora/precomputed/tf_gammas.pt
```
The main fine-tuning experiments are located in the finetuning_enformer directory.
Before launching experiments, please update all dataset and checkpoint paths in:
```bash
train_enformer/config.py
```
This file contains clearly organized fields for all file paths required during training,
including real-data inputs, synthetic-data inputs. You must replace these with paths valid for your system.
All required input files are located in the data/ directory, making it easy to match
each field in config.py with the corresponding dataset.
In particular, the fine-tuning pipeline requires the outputs of:
```bash
data_preprocessing/1.preparing_real_sequences/3.fasta_to_vector.py
data_preprocessing/2.preparing_synthetic_sequences/9.fasta_to_vector.py
```


To launch all experiments in the suite simultaneously (as run in our study on 4√óH100 GPUs):
```bash
deepspeed --num_nodes 1 --num_gpus 4 \
    --master_addr ${MASTER_ADDR} \
    --master_port ${MASTER_PORT} \
    --module train_enformer.main -- --suite
```
Environment variables MASTER_ADDR and MASTER_PORT can be set manually if needed
(e.g., when multiple distributed jobs are running on the same machine).

This command automatically executes all experiment phases, including:

‚Ä¢ Real-only regression

‚Ä¢ Real‚Äìsynthetic alternating fine-tuning

‚Ä¢ Real-only fine-tuning combining regression and Bradley‚ÄìTerry preference objectives


## üìä Evaluation
After completing a training experiment, you can evaluate the model on the test set by running:
```bash
python evaluation_on_testset.py
```
This script loads the best-performing checkpoint (selected based on validation Pearson correlation), computes per-gene Pearson and Spearman correlations between predicted and observed expression values across individuals, and outputs both gene-level and summary metrics.


## üìÑ Citation & Licenses

This repository integrates components from several open-source projects under the MIT License:
```bash
Copyright (c) 2018 Kipoi team
Licensed under the MIT License.
Source: https://github.com/kipoi/kipoiseq

Copyright (c) 2021 Phil Wang
Licensed under the MIT License.
Source: https://github.com/lucidrains/enformer-pytorch

Copyright (c) 2023 ni-lab
Licensed under the MIT License.
Source: https://github.com/ni-lab/personalized-expression-benchmark
```

All sequences used in this study were derived from [E-GEUV-1](https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-GEUV-1). The synthetic genomes generated using sim1000G were also simulated based on this dataset as input.
